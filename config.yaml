---
'data':
  'train_path' : 'data/train_data_2^18.npy'
  'test_path' : 'data/test_data_2^18.npy'
  'base' : 24 # base the number is factored in

'model_args':
  'embed_dim' : 128
  'num_encoder_layers' : 6
  'num_decoder_layers' : 6
  'dim_feedforward' : 512
  'dropout' : .05
  'shared_embeddings' : True
  'scale_embeddings' : False
  'learn_positional_encoding' : False
  'max_decode_size' : 64

'optimizer':
  'type' : 'adam'
  'opt_args' : 
    'lr' : 0.001
  'max_grad_norm' : 1
  'gradient_accumulation_steps' : 1

'scheduler' : 
  'type' : linear_schedule_with_warmup
  'n_warmup_steps' : 24000
  'nb_epochs' : 200
  'max_steps' : -1

'loader' :
  'train' : 
    'batch_size' : 64
    'random_sampling' : True # random_sampling is slightly different than shuffle b/c it allows same # to be sampled multiple times.
                             # Benefit is that we don't need to store the numbers that've been sampled in memory --> use instead for large data sizes 
                             # (empircally don't go too much higher than using shuffle instead 2**27)

  'test' : 
    'batch_size' : 256
    'shuffle' : False

'io':
  'save_path' : './models/base_24_2^18/'
  'checkpoint_every' : 10000
  'evaluate_every' : 1
  'evaluate_final' : True

'metrics' : 
  'n_beams' : 2
  'max_num' : -1
  'save_suffix' : ''
  'temperature' : 1.0

'multi_gpu' : 'auto'



'verbose' : True