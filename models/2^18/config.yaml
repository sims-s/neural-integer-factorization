data: {base: 24, test_path: data//test_data_2^18.npy, train_path: data//train_data_2^18.npy}
io: {checkpoint_every: 10000, evaluate_every: 1, evaluate_final: true, save_path: ../gdrive/MyDrive/models/factorization/2^18/}
loader:
  test: {batch_size: 256, num_workers: 4, shuffle: false}
  train: {batch_size: 128, num_workers: 4, random_sampling: true}
metrics: {max_num: -1, n_beams: 2, save_suffix: '', temperature: 1.0}
model_args: {dim_feedforward: 2048, dropout: 0.1, embed_dim: 128, learn_positional_encoding: false,
  max_decode_size: 64, num_decoder_layers: 6, num_encoder_layers: 6, scale_embeddings: false,
  shared_embeddings: true}
multi_gpu: false
optimizer:
  gradient_accumulation_steps: 1
  max_grad_norm: 1
  opt_args: {lr: 0.001}
  type: adam
resume_training: true
scheduler: {max_steps: -1, n_warmup_steps: 24000, nb_epochs: 200, nb_steps: 368600,
  type: linear_schedule_with_warmup}
tokenizer: {n_tokens: 28, pad_token_id: 25}
verbose: true
