data:
  base: 7
  oos_path: data/oos_data_2^16.npy
  test_path: data/test_data_2^16.npy
  train_path: data/train_data_2^16.npy
io:
  checkpoint_every: 10000
  evaluate_every: 1
  evaluate_final: true
  save_path: ./models/baseline_base7/
loader:
  oos:
    batch_size: 256
    shuffle: false
  test:
    batch_size: 256
    shuffle: false
  train:
    batch_size: 64
    random_sampling: true
metrics:
  max_num: -1
  n_beams: 2
  save_suffix: ''
  temperature: 1.0
model_args:
  attn_weight_xavier_init_constant: 0.5
  dim_feedforward: 512
  dropout: 0.05
  embed_dim: 128
  extra_positional_encoding_relative_decoder_mha: true
  learn_positional_encoding: false
  max_decode_size: 64
  norm_first: false
  num_decoder_layers: 6
  num_encoder_layers: 6
  positional_encoding_query_key_only: false
  relative_positional_encoding: true
  repeat_positional_encoding: true
  scale_embeddings: false
  scale_embeddings_at_init: false
  shared_embeddings: true
optimizer:
  gradient_accumulation_steps: 1
  max_grad_norm: 1
  opt_args:
    lr: 0.001
  type: adam
resume_training: false
scheduler:
  max_steps: -1
  n_warmup_steps: 24000
  nb_epochs: 200
  nb_steps: 184400
  type: linear_schedule_with_warmup
tokenizer:
  n_tokens: 11
  pad_token_id: 8
verbose: true
wandb:
  enabled: true
  entity: sims-s
  id: 195mour2
  project: integer-factorization
  watch_args:
    log: all
    log_freq: 50
