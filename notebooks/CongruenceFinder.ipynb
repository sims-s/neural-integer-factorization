{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "from tokenizer import Tokenizer\n",
    "from data_utils import dec2base, base2dec\n",
    "from models import TransformerEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CongruenceDataset(Dataset):\n",
    "    def __init__(self, max_val, base, skip_nums=None, sample_nums=None, sample_func=None, size=-1):\n",
    "        self.max_val = max_val\n",
    "        self.base = base\n",
    "        \n",
    "        if skip_nums is not None:\n",
    "            self.skip_nums = set(skip_nums)\n",
    "        else:\n",
    "            self.skip_nums = set([])\n",
    "            \n",
    "        if sample_nums is None:\n",
    "            self.sample_nums = None\n",
    "        else:\n",
    "            self.sample_nums = sample_nums\n",
    "        \n",
    "        if sample_func is None:\n",
    "            if sample_nums is None:\n",
    "                self.sample_func = self.default_sample_func\n",
    "            else:\n",
    "                self.sample_func = lambda i: self.sample_nums[i]\n",
    "        else:\n",
    "            self.sample_func = sample_func\n",
    "            \n",
    "        if size < 0:\n",
    "            self.size = self.max_val\n",
    "        else:\n",
    "            self.size = size\n",
    "            \n",
    "    def default_sample_func(self, i):\n",
    "        while i in self.skip_nums:\n",
    "            i = np.random.randint(0, self.max_val)\n",
    "        return i\n",
    "            \n",
    "    def __getitem__(self, i):\n",
    "        return dec2base(self.sample_func(i), base)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if self.sample_nums is not None:\n",
    "            return len(self.sample_nums)\n",
    "        else:\n",
    "            return self.size\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = 2**16\n",
    "test_size = 5000\n",
    "base = 10\n",
    "test_nums = np.random.randint(0, max_val, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CongruenceDataset(max_val, base, skip_nums = test_nums, size=10000)\n",
    "test_dataset = CongruenceDataset(max_val, base, sample_nums = test_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn = lambda x: list(x))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn = lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 6,  5,  0,  5, 11],\n",
      "        [ 1,  1,  4,  0, 11],\n",
      "        [ 1,  9,  6,  6, 11],\n",
      "        [ 9,  2,  2,  0, 11],\n",
      "        [ 3,  2,  0,  7, 11],\n",
      "        [ 4,  9,  7,  9, 11],\n",
      "        [ 8,  6,  4,  8, 11],\n",
      "        [ 8,  4,  6,  9, 11],\n",
      "        [ 6,  9,  1,  7, 11],\n",
      "        [ 4,  5, 11, 11, 11],\n",
      "        [ 3,  6,  6,  8,  3],\n",
      "        [ 6,  1,  2,  5, 11],\n",
      "        [ 6,  7,  4,  6, 11],\n",
      "        [ 6,  0,  1, 11, 11],\n",
      "        [ 6,  1,  2,  7, 11],\n",
      "        [ 3,  2,  2,  4, 11]])\n",
      "torch.Size([16, 5])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    tokenized = torch.tensor(t.encode(batch))\n",
    "    print(tokenized)\n",
    "    print(tokenized.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "n_chunks = int(np.ceil(np.log(2**16) / np.log(base))) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CongruenceFinder(nn.Module):\n",
    "    def __init__(self, n_tokens, n_chunks, embed_dim, **transformer_args):\n",
    "        super(CongruenceFinder, self).__init__()\n",
    "        self.embedding = TransformerEmbedding(n_tokens, embed_dim, 32, .1, False, False)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, 8, **transformer_args)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, 4, norm=nn.LayerNorm(embed_dim))\n",
    "        self.pred_out = nn.Linear(embed_dim, n_chunks*2)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        encoded = self.transformer(x)\n",
    "        encoded = encoded.transpose(0,1).mean(dim=1)\n",
    "        pred = self.pred_out(encoded)\n",
    "        print('pred size: ', pred.size())\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CongruenceFinder(len(t), n_chunks, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits):\n",
    "    probs = torch.sigmoid(logits).detach().data.cpu().numpy()\n",
    "    scale_vector = np.array([base**i for i in range(n_chunks)])\n",
    "    \n",
    "    first_num = probs[:,:n_chunks] @ scale_vector\n",
    "    second_num = probs[:,n_chunks:] @ scale_vector\n",
    "    \n",
    "    first_squared = first_num**2\n",
    "    second_squared = second_num**2\n",
    "    \n",
    "    print(first_squared.shape, second_squared.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred size:  torch.Size([16, 12])\n",
      "torch.Size([16, 12])\n",
      "(16,) (16,)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    tokenized = torch.tensor(t.encode(batch))\n",
    "    logits = model(tokenized)\n",
    "    print(logits.size())\n",
    "    loss_fn(logits)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
