{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../src/PrimeFactorization/')\n",
    "from PrimeFactorization import primeFactorize\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import os\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "import pandas as pd\n",
    "import json\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tokenizer.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_power = 16\n",
    "# 2*x power to accomodate for 100=10*10 + 1x power - 1 for multiplication symbols + 1 for end of sequence\n",
    "max_input_size = max_power+1\n",
    "max_decode_size = 3*max_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=65534.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(total = 2**max_power - 2)\n",
    "for i in range(2, 2**max_power):\n",
    "    data[i] = primeFactorize(i)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65534"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_factor_mapping = copy.deepcopy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(number):\n",
    "    try:\n",
    "        factored = global_factor_mapping[number]\n",
    "        return len(factored)==1 and factored[list(factored.keys())[0]]==1\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "test_data = {}\n",
    "train_counter = 0\n",
    "test_counter = 0\n",
    "\n",
    "train_prob = .8\n",
    "np.random.seed(0)\n",
    "for number, factor in data.items():\n",
    "    if np.random.rand() < train_prob:\n",
    "        train_data[train_counter] = {'number' : number, 'factors' : factor} \n",
    "        train_counter +=1\n",
    "    else:\n",
    "        test_data[test_counter] = {'number' : number, 'factors' : factor} \n",
    "        test_counter +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52517, 13017)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = binarize_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {}\n",
    "test_data = {}\n",
    "train_counter = 0\n",
    "test_counter = 0\n",
    "\n",
    "train_prob = .8\n",
    "np.random.seed(0)\n",
    "for number, factor in data.items():\n",
    "    if np.random.rand() < train_prob:\n",
    "        train_data[train_counter] = {'number' : number, 'factors' : factor} \n",
    "        train_counter +=1\n",
    "    else:\n",
    "        test_data[test_counter] = {'number' : number, 'factors' : factor} \n",
    "        test_counter +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = FactorizationDataset(train_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = FactorizationDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset.data_dict[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # From: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "    def __init__(self, d_model, dropout, max_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerEmbedding(nn.Module):\n",
    "    def __init__(self, n_tokens, embed_dim, dropout):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_tokens, embed_dim)\n",
    "        self.pe = PositionalEncoding(embed_dim, dropout, max_decode_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).transpose(0,1)\n",
    "        x = self.pe(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waht other junk goes on when  training a transfoerm? learning rate schedulign?\n",
    "## MORE DATA\n",
    "# optimize Decode\n",
    "# Harden --> github\n",
    "# Increase bathc size\n",
    "# Gradient clipping can too\n",
    "# Make entireley configurable based on a config\n",
    "# Gradietn accumulation\n",
    "# Script to generate data\n",
    "# evaluation scrip9t\n",
    "# config should have an overwrite function. Clears directory if yes. Otherwise break\n",
    "# Deal with stupid bug in metrics......\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Factorizer(nn.Module):\n",
    "    def __init__(self, n_tokens, embed_dim, shared_embeddings, **kwargs):\n",
    "        super(Factorizer, self).__init__()\n",
    "        self.shared_embeddings = shared_embeddings\n",
    "        dropout = .1 if not 'dropout' in kwargs else kwargs['dropout']\n",
    "        if not shared_embeddings:\n",
    "            self.src_embedding = TransformerEmbedding(n_tokens, embed_dim, dropout)\n",
    "            self.tgt_embedding = TransformerEmbedding(n_tokens, embed_dim, dropout)\n",
    "        else:\n",
    "            self.embedding = TransformerEmbedding(n_tokens, embed_dim, dropout)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model = embed_dim, **kwargs)\n",
    "        \n",
    "        self.tokens_out = nn.Linear(embed_dim, n_tokens)\n",
    "        \n",
    "    def form_pad_mask(self, tokens):\n",
    "        return (tokens==tokenizer('_')[0]).to(tokens.device)\n",
    "    \n",
    "    def form_subsequence_mask(self, tgt):\n",
    "        size = tgt.size(0)\n",
    "        return (torch.triu(torch.ones(size, size)) == 0).transpose(0,1).to(tgt.device)\n",
    "    \n",
    "    def route_embeddings(self, src_or_tgt, input_type):\n",
    "        if self.shared_embeddings:\n",
    "            return self.embedding(src_or_tgt)\n",
    "        if input_type=='src':\n",
    "            return self.src_embedding(src_or_tgt)\n",
    "        elif input_type=='tgt':\n",
    "            return self.tgt_embedding(src_or_tgt)\n",
    "    \n",
    "    def encode(self, src):\n",
    "        src_key_padding_mask = self.form_pad_mask(src)\n",
    "        src = self.route_embeddings(src, 'src')\n",
    "        \n",
    "        return self.transformer.encoder(src, src_key_padding_mask = src_key_padding_mask), src_key_padding_mask\n",
    "    \n",
    "    def decode(self, tgt, memory, memory_key_padding_mask):\n",
    "        tgt_key_padding_mask = self.form_pad_mask(tgt)\n",
    "        tgt = self.route_embeddings(tgt, 'tgt')\n",
    "        \n",
    "        tgt_mask = self.form_subsequence_mask(tgt)\n",
    "        \n",
    "        output = self.transformer.decoder(tgt, memory, tgt_mask = tgt_mask, \n",
    "                                          tgt_key_padding_mask = tgt_key_padding_mask,\n",
    "                                          memory_key_padding_mask = memory_key_padding_mask)\n",
    "        output = output.transpose(0,1)\n",
    "        decoded = self.tokens_out(output)\n",
    "        return decoded\n",
    "    \n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "        memory, memory_key_padding_mask = self.encode(src)\n",
    "        decoded = self.decode(tgt, memory, memory_key_padding_mask)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = {\n",
    "    'n_tokens' : len(tokenizer),\n",
    "    'embed_dim' : 64,\n",
    "    'num_encoder_layers' : 8,\n",
    "    'num_decoder_layers' : 8,\n",
    "    'dim_feedforward' : 128,\n",
    "    'dropout' : .1,\n",
    "    'shared_embeddings' : True,\n",
    "}\n",
    "opt_args = {\n",
    "    'n_warmup_steps' : 2000,\n",
    "    'nb_epochs' : 50,\n",
    "}\n",
    "opt_args['total_steps'] = opt_args['nb_epochs'] * len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = '../models/pad_instead_of_zero/'\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_file(path):\n",
    "    files = [f for f in os.listdir(path) if f.endswith('.pt')]\n",
    "    original_files = files\n",
    "    files = [f.split('_')[1] for f in files]\n",
    "    losses = [float('.'.join(f.split('.')[:-1])) for f in files]\n",
    "    try:\n",
    "        best_loss_idx = np.argmin(losses)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    return path + original_files[best_loss_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = save_path + \"51_0.1157.pt\"\n",
    "checkpoint_path = get_best_model_file(save_path)\n",
    "# checkpoint_path = None\n",
    "checkpoint_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if checkpoint_path:\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model = Factorizer(**checkpoint['model_parameters'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    opt = optim.Adam(model.parameters())\n",
    "    opt.load_state_dict(checkpoint['opt'])\n",
    "    model.to(device)\n",
    "#     scheduler = get_linear_schedule_with_warmup(opt, checkpoint['opt_args']['n_warmup_steps'], checkpoint['opt_args']['total_steps'], last_epoch=checkpoint['opt']['epoch'])\n",
    "else:\n",
    "    model = Factorizer(**model_parameters).to(device)\n",
    "    opt = optim.Adam(model.parameters())\n",
    "    scheduler = get_linear_schedule_with_warmup(opt, opt_args['n_warmup_steps'], opt_args['total_steps'])\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch(batch, backward=True):\n",
    "    numbers = torch.tensor(tokenizer(batch['number'])).to(device)\n",
    "    labels = torch.tensor(tokenizer(batch['label'])).to(device)\n",
    "    res = model(numbers, labels[:,:-1])\n",
    "    loss = loss_func(res.view(-1, 5), labels[:,1:].reshape(-1))\n",
    "    if backward:\n",
    "        loss.backward()\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 1, 41: 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.is_prime(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_epoch(loader):\n",
    "    model.train()\n",
    "    pbar = tqdm(total = len(loader), leave=False)\n",
    "    loss_hist = []\n",
    "    for i, batch in enumerate(loader):\n",
    "        model.zero_grad()\n",
    "        loss = run_batch(batch, backward=True)\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        pbar.update(1)\n",
    "        loss_hist.append(loss)\n",
    "    pbar.close()\n",
    "    mean_loss = np.mean(loss_hist)\n",
    "    \n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_for_epoch(loader):\n",
    "    model.eval()\n",
    "    pbar = tqdm(total = len(loader), leave=False)\n",
    "    loss_hist = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            loss_hist.append(run_batch(batch, backward=False))\n",
    "            pbar.update(1)\n",
    "    pbar.close()\n",
    "    return np.mean(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pbar = tqdm(total = opt_args['nb_epochs'])\n",
    "# for i in range(opt_args['nb_epochs'] ):\n",
    "#     train_loss = train_for_epoch(train_loader)\n",
    "#     test_loss = test_for_epoch(test_loader)\n",
    "    \n",
    "#     if save_path:\n",
    "#         torch.save({'epoch' : i, \n",
    "#                     'state_dict' : model.state_dict(), \n",
    "#                     'opt': opt.state_dict(), \n",
    "#                     'train_loss' : train_loss, \n",
    "#                     'test_loss' : test_loss,\n",
    "#                     'model_parameters' : model_parameters,\n",
    "#                     'opt_args': opt_args,\n",
    "#                     'epoch' : i},\n",
    "#                     '%s/%d_%.4f.pt'%(save_path, i, test_loss))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('Train: %.6f, Test: %.6f'%(train_loss, test_loss))\n",
    "#     pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(memory, memory_key_padding_mask, n_beams=2):\n",
    "    sequences = torch.tensor(tokenizer('1')).to(device).unsqueeze(0)\n",
    "    sequence_log_probs = torch.tensor([[0]]).to(device)\n",
    "    stop_char = tokenizer('.')[0]\n",
    "    \n",
    "    for i in range(max_decode_size-1):\n",
    "        with torch.no_grad():\n",
    "            output = model.decode(sequences, \n",
    "                                  memory.repeat(1,sequences.size(0),1), \n",
    "                                  memory_key_padding_mask.repeat(sequences.size(0), 1))[:,-1]\n",
    "#                                   memory_key_padding_mask)[:,-1]\n",
    "            output = torch.log_softmax(output, dim=-1)\n",
    "        \n",
    "        # Compue all possible next sequences\n",
    "        sequences = torch.repeat_interleave(sequences, len(tokenizer), 0)\n",
    "        next_tokens = torch.arange(len(tokenizer)).repeat(output.size(0)).unsqueeze(1).to(sequences.device)\n",
    "        next_sequences = torch.cat((sequences, next_tokens), dim=1)\n",
    "        \n",
    "        # Get the indiclies of the highest probability sequences\n",
    "        next_token_log_probs = output.view(-1,1)\n",
    "        # For cases when the model migth predict a non padding token after an end of sequence token\n",
    "        # Manually set the log probability to be very low so it's never chosen to be decdoed\n",
    "        # Additionally, break if all sequences have eos tokens\n",
    "        seq_has_eos = torch.argmax((sequences==tokenizer('.')[0]).int(), dim=1).view(-1,1) > 0\n",
    "        if torch.all(seq_has_eos):\n",
    "            # undo repeat_interleave\n",
    "            sequences = sequences[::len(tokenizer)]\n",
    "            break\n",
    "\n",
    "        # Compute all possible next log probabilities\n",
    "        next_token_log_probs[seq_has_eos & ~(next_tokens==tokenizer('_')[0])] = -np.log(10000)\n",
    "        sequence_log_probs = torch.repeat_interleave(sequence_log_probs, len(tokenizer), 0)\n",
    "        next_sequence_log_probs = sequence_log_probs + next_token_log_probs\n",
    "        \n",
    "        top_indicies = torch.argsort(next_sequence_log_probs, dim=0, descending=True).squeeze()[:n_beams]\n",
    "        \n",
    "        sequences = next_sequences[top_indicies]\n",
    "        sequence_log_probs = next_sequence_log_probs[top_indicies]\n",
    "    \n",
    "    sequences = sequences.data.cpu().numpy()\n",
    "    sequence_log_probs = sequence_log_probs.data.cpu().numpy()\n",
    "    return sequences, sequence_log_probs\n",
    "\n",
    "\n",
    "def compute_full_target_str(base_10_number):\n",
    "    number = dec2bin(base_10_number)\n",
    "    factors = {dec2bin(k) : v for k, v in global_factor_mapping[base_10_number].items()}\n",
    "    tmp_ds = FactorizationDataset({0:{'number' : number, 'factors' : factors}})\n",
    "    return tmp_ds[0]['label']\n",
    "        \n",
    "def postprocess(factor_list, log_prob, base_10_number, number, beam_idx):\n",
    "    tokenized = tokenizer(factor_list)\n",
    "    \n",
    "    information = {\n",
    "        'target_num' : base_10_number,\n",
    "        'target_is_prime' : is_prime(base_10_number),\n",
    "        'target_str_full' : compute_full_target_str(base_10_number),\n",
    "        'target_factor_list' : sum([[k]*v for k, v in global_factor_mapping[base_10_number].items()], []),\n",
    "        'pred_str_full' : ''.join(tokenizer(factor_list, decode_special=True)) + '_'*(max_decode_size-len(factor_list)),\n",
    "        'pred_str' : ''.join(tokenized),\n",
    "        'beam_idx' : beam_idx,\n",
    "        'log_prob' : log_prob.item(),\n",
    "    }\n",
    "    information['n_target_factors'] = len(information['target_factor_list'])\n",
    "    \n",
    "    factor_list = tokenized.split('x')\n",
    "    try:\n",
    "        factors = [bin2dec(num) for num in factor_list]\n",
    "    except ValueError:\n",
    "        factors = []\n",
    "    \n",
    "    information['pred_factor_list'] = factors\n",
    "    information['n_pred_factors'] = len(information['pred_factor_list'])\n",
    "    if len(information['pred_factor_list']) > 0:\n",
    "        information['product'] = np.prod(factors)\n",
    "    else:\n",
    "        information['product'] = np.nan\n",
    "    information['correct_product'] = information['product']==base_10_number\n",
    "    information['correct_factorization'] = information['correct_product'] & all([is_prime(n) for n in information['pred_factor_list']])\n",
    "    \n",
    "    information['num_prime_factors_pred'] = np.sum([len(global_factor_mapping[f])==1 for f in factors if f in global_factor_mapping])\n",
    "    information['percent_prime_factors_pred'] = information['num_prime_factors_pred'] / information['n_pred_factors']\n",
    "        \n",
    "    \n",
    "    target_str_full = information['target_str_full']\n",
    "    pred_str_full = information['pred_str_full']\n",
    "    information['seq_dist_binary'] = 1-np.mean([c1==c2 for c1, c2 in zip(target_str_full, pred_str_full)])\n",
    "    information['seq_dist_lev'] = levenshtein_distance(target_str_full, pred_str_full)\n",
    "    return information\n",
    "    \n",
    "\n",
    "        \n",
    "def factor(number, n_beams=1, return_type='df'):\n",
    "    base_10_num = number\n",
    "    model.eval()\n",
    "    \n",
    "    # Conver the number to a tensor\n",
    "    number = dec2bin(number)\n",
    "    number = tokenizer(pad_input(number))\n",
    "    number = torch.tensor(number).unsqueeze(0).to(device)\n",
    "    # Encode the number\n",
    "    with torch.no_grad():\n",
    "        memory, memory_key_padding_mask = model.encode(number)\n",
    "    # Decode!\n",
    "    factors_list, log_probs = decode(memory, memory_key_padding_mask, n_beams)\n",
    "    number = number.data.cpu().numpy()[0]\n",
    "    to_return = []\n",
    "    for i in range(len(factors_list)):\n",
    "        to_return.append(postprocess(factors_list[i], log_probs[i], base_10_num, number, i))\n",
    "        \n",
    "    if return_type=='df':\n",
    "        return pd.DataFrame.from_dict(to_return)\n",
    "    elif return_type=='dict':\n",
    "        return to_return\n",
    "    else:\n",
    "        raise ValueError('got unexpected return type %s'%return_type)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_metric_df(items, n_beams=1, max_num=-1):\n",
    "    rows = []\n",
    "    \n",
    "    pbar = tqdm(total = min(len(items), max_num) if max_num > 0 else len(items), leave=False)\n",
    "    for i, num in enumerate(items):\n",
    "        if max_num > 0 and i >= max_num:\n",
    "            break\n",
    "        if num < 2: continue\n",
    "        rows.append(factor(num, n_beams, return_type='dict'))\n",
    "        pbar.update(1)\n",
    "    pbar.update(2)\n",
    "    pbar.close()\n",
    "    rows = sum(rows, [])\n",
    "    df = pd.DataFrame.from_dict(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor(45165, n_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factor(1221, n_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "factor(127, n_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "factor(456, n_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor(4, n_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numbers  = []\n",
    "for k, v in test_dataset.data_dict.items():\n",
    "    test_numbers.append(bin2dec(v['number']))\n",
    "len(test_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(get_best_model_file(save_path))['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_df = form_metric_df(test_numbers, n_beams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.to_pickle(save_path + 'metric_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(metric_df):\n",
    "    metrics = {}\n",
    "    grouped_by_num = metric_df.groupby('target_num')\n",
    "    metrics['correct'] = grouped_by_num.agg({\n",
    "        'correct_product' : 'any',\n",
    "        'correct_factorization' : 'any'\n",
    "    }).mean(axis=0).to_dict()\n",
    "    \n",
    "    metrics['beam_accuracy'] = metric_df.groupby('beam_idx').agg({\n",
    "        'correct_product' : 'mean',\n",
    "        'correct_factorization' : 'mean'\n",
    "    }).to_dict()\n",
    "    \n",
    "    metric_df['log_prob_decile'] = pd.qcut(metric_df['log_prob'], q=10).apply(str)\n",
    "    metrics['by_prob'] = metric_df.groupby('log_prob_decile').agg({\n",
    "        'correct_product': 'mean',\n",
    "        'correct_factorization' : 'mean',\n",
    "        'seq_dist_binary' : 'mean',\n",
    "        'seq_dist_lev' : 'mean',\n",
    "        'percent_prime_factors_pred' : 'mean',\n",
    "    }).to_dict()\n",
    "    \n",
    "    \n",
    "    # Things about the target, we want to take the first one of. Otherwise, we want all of them\n",
    "    grouped_by_num = grouped_by_num.agg({k: 'first' if 'target' in k else list for k in list(metric_df) if not k=='target_num'}).reset_index()\n",
    "    \n",
    "    metrics['by_n_target_factors'] = grouped_by_num.groupby('n_target_factors').agg({\n",
    "        'correct_product' : [lambda x: pd.Series([np.mean([any(y) for y in x])]), 'size'],\n",
    "        'correct_factorization' : [lambda x: pd.Series([np.mean([any(y) for y in x])]), 'size']\n",
    "    })\n",
    "    metrics['by_n_target_factors'].columns = ['correct_product_mean', 'correct_product_size', 'correct_factorization_mean', 'correct_factorization_size']\n",
    "    metrics['by_n_target_factors'] = metrics['by_n_target_factors'].to_dict()\n",
    "    \n",
    "    grouped_by_num['number_decile'] = pd.qcut(grouped_by_num['target_num'], q=10).apply(str)\n",
    "    metrics['by_target_number'] = grouped_by_num.groupby('number_decile').agg({\n",
    "        'correct_product' : [lambda x: pd.Series([np.mean([any(y) for y in x])]), 'size'],\n",
    "        'correct_factorization' : [lambda x: pd.Series([np.mean([any(y) for y in x])]), 'size']\n",
    "    })\n",
    "    metrics['by_target_number'].columns = ['correct_product_mean', 'correct_product_size', 'correct_factorization_mean', 'correct_factorization_size']\n",
    "    metrics['by_target_number'] = metrics['by_target_number'].to_dict()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path, to_save):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(to_save, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(metric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_json(save_path + 'metrics.json', metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df.groupby('target_num').agg({'correct_product' : 'any'}).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
