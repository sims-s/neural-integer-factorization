{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import yaml\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', 999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '../models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = []\n",
    "metric_list = []\n",
    "\n",
    "def find_metrics_in_dir(base_path):\n",
    "    for f in os.listdir(base_path):\n",
    "        print(f)\n",
    "        if f=='.gitignore': continue\n",
    "        subdir_path = base_path + f + '/'\n",
    "        \n",
    "        if os.path.exists(subdir_path + 'checkpoints/'):\n",
    "            config_path = subdir_path + 'config.yaml'\n",
    "            metrics_path = subdir_path + 'metrics_test.json'\n",
    "\n",
    "            config_list.append(load_yaml(config_path))\n",
    "            metric_list.append(load_json(metrics_path))\n",
    "            \n",
    "        elif os.path.isdir(subdir_path) and not f=='checkpoints':\n",
    "            find_metrics_in_dir(subdir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_metrics_in_dir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(config_list), len(metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_config_item(config_item):\n",
    "    expanded = {}\n",
    "    expanded['base'] = config_item['data']['base']\n",
    "    expanded['train_batch_size'] = config_item['loader']['train']['batch_size']\n",
    "    for k, v in config_item['model_args'].items():\n",
    "        expanded[k] = v\n",
    "    expanded['optimizer'] = config_item['optimizer']['type']\n",
    "    for k, v in config_item['optimizer']['opt_args'].items():\n",
    "        expanded[k] = v\n",
    "#     handle all model args\n",
    "#     handle all opt args\n",
    "    expanded['n_warmup_steps'] = config_item['scheduler']['n_warmup_steps']\n",
    "    expanded['nb_epochs'] = config_item['scheduler']['nb_epochs']\n",
    "    expanded['max_grad_norm'] = config_item['optimizer']['max_grad_norm']\n",
    "    expanded['learn_positional_encoding'] = config_item['model_args']['learn_positional_encoding']\n",
    "    return expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded = [expand_config_item(c) for c in config_list]\n",
    "config_df = pd.DataFrame.from_dict(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nice_metrics(metric_list):\n",
    "    correct = pd.DataFrame.from_dict([l['correct'] for l in metric_list])\n",
    "    n_beams = pd.DataFrame.from_dict([l['meta']['n_beams'] for l in metric_list])\n",
    "    n_beams.columns = ['n_beams']\n",
    "    \n",
    "    return [correct, n_beams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([config_df] + get_nice_metrics(metric_list), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all confiuraiton columns that have no variantion b/c that's not super helpful\n",
    "drop_cols = []\n",
    "for c in list(config_df) + ['n_beams']:\n",
    "    if merged[c].nunique()==1:\n",
    "        drop_cols.append(c)\n",
    "metric_df = merged.drop(drop_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_value(df, groupby_col, metric_cols, metrics=None, num_columns = 4):\n",
    "    if not metrics:\n",
    "        metrics = {c: 'mean' for c in metric_cols}\n",
    "    elif isinstance(metrics, list):\n",
    "        metrics = {c: m for c, m in enumerate(metric_cols, metrics)}\n",
    "    \n",
    "    num_metrics = len(metric_cols)\n",
    "    num_rows = num_metrics // num_columns + num_metrics % num_columns\n",
    "    if num_rows==1:\n",
    "        num_columns = num_metrics\n",
    "        \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8*num_rows, 2*num_columns)\n",
    "    grouped_by_target = df.groupby(groupby_col)\n",
    "    for i, (metric, function) in enumerate(metrics.items()):\n",
    "        ax = fig.add_subplot(num_rows, num_columns, 1+i)\n",
    "        ax.set_title('%s'%metric)\n",
    "        ax.set_ylabel('%s'%metric)\n",
    "        grouped_by_target.agg({metric : function}).plot(ax=ax, legend=False, marker='x')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_crossed_values(df, groupby_col, cross_col, metric_cols, metrics=None, num_columns = 4):\n",
    "    if not metrics:\n",
    "        metrics = {c: 'mean' for c in metric_cols}\n",
    "    elif isinstance(metrics, list):\n",
    "        metrics = {c: m for c, m in enumerate(metric_cols, metrics)}\n",
    "    \n",
    "    num_metrics = len(metric_cols)\n",
    "    num_rows = num_metrics // num_columns + num_metrics % num_columns\n",
    "    if num_rows==1:\n",
    "        num_columns = num_metrics\n",
    "        \n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8*num_rows, 2*num_columns)\n",
    "    \n",
    "    \n",
    "    grouped_by_cross = df.groupby(cross_col)\n",
    "\n",
    "    for i, (metric, function) in enumerate(metrics.items()):\n",
    "        ax = fig.add_subplot(num_rows, num_columns, 1+i)\n",
    "        ax.set_title('%s'%metric)\n",
    "        ax.set_ylabel('%s'%metric)\n",
    "        for cross_name, cross_df in grouped_by_cross:\n",
    "            cross_df = cross_df.groupby(groupby_col).agg('mean')\n",
    "#             cross_df.set_index(groupby_col, inplace=True)\n",
    "            cross_df[metric].plot(ax=ax, legend=False, label='%s: '%cross_col + str(cross_name), marker='x')\n",
    "        ax.legend()\n",
    "        \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_cols = ['correct_product', 'correct_factorization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_crossed_values(metric_df, 'base', 'n_beams', metric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_value(metric_df, 'base', metric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
